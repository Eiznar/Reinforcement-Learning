{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oO-THVYxa37l"
      },
      "source": [
        "# AlphaZero Tic Tac Toe\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A544uUAlcTk5"
      },
      "source": [
        "## Game rules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJF3J_YjawCk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class TicTacToeGame:\n",
        "    \"\"\"\n",
        "        rows: 3\n",
        "        columns: 3\n",
        "        winNumber: 3\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.rows = 3\n",
        "        self.columns = 3\n",
        "        self.win = 3\n",
        "\n",
        "    def get_init_board(self):\n",
        "        b = np.zeros((self.rows, self.columns), dtype=np.int)\n",
        "        return b\n",
        "\n",
        "    def get_board_size(self):\n",
        "        return (self.rows, self.columns)\n",
        "\n",
        "    def get_action_size(self):\n",
        "        return self.rows * self.columns\n",
        "\n",
        "    def get_next_state(self, board, player, action):\n",
        "        b = np.copy(board)\n",
        "        row, col = divmod(action, self.columns)\n",
        "        b[row][col] = player\n",
        "        return (b, -player)\n",
        "\n",
        "    def has_legal_moves(self, board):\n",
        "        for row in range(self.rows):\n",
        "            for col in range(self.columns):\n",
        "                if board[row][col] == 0:\n",
        "                    return True\n",
        "        return False\n",
        "\n",
        "    def get_valid_moves(self, board):\n",
        "        valid_moves = [1 if board[i][j] == 0 else 0 for i in range(self.rows) for j in range(self.columns)]\n",
        "        return valid_moves\n",
        "\n",
        "    def is_win(self, board, player):\n",
        "        # check rows\n",
        "        for row in range(self.rows):\n",
        "            if all(board[row][col] == player for col in range(self.columns)):\n",
        "                return True\n",
        "        # check columns\n",
        "        for col in range(self.columns):\n",
        "            if all(board[row][col] == player for row in range(self.rows)):\n",
        "                return True\n",
        "        # check diagonals\n",
        "        if all(board[i][i] == player for i in range(self.rows)):\n",
        "            return True\n",
        "        if all(board[i][self.rows-i-1] == player for i in range(self.rows)):\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def get_reward_for_player(self, board, player):\n",
        "        if self.is_win(board, player):\n",
        "            return 1\n",
        "        if self.is_win(board, -player):\n",
        "            return -1\n",
        "        if self.has_legal_moves(board):\n",
        "            return None\n",
        "        else : \n",
        "          return 0\n",
        "\n",
        "    def get_canonical_board(self, board, player):\n",
        "        return player * board\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-xKeu7zczn7"
      },
      "source": [
        "## Monte Carlo Tree Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmeIx5t-dJ4H"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def ucb_score(parent, child):\n",
        "    \"\"\"\n",
        "    The score for an action that would transition between the parent and child.\n",
        "    \"\"\"\n",
        "    prior_score = child.prior * math.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
        "    if child.visit_count > 0:\n",
        "        # The value of the child is from the perspective of the opposing player\n",
        "        value_score = -child.value()\n",
        "    else:\n",
        "        value_score = 0\n",
        "\n",
        "    return value_score + prior_score\n",
        "\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, prior, to_play):\n",
        "        self.visit_count = 0\n",
        "        self.to_play = to_play\n",
        "        self.prior = prior\n",
        "        self.value_sum = 0\n",
        "        self.children = {}\n",
        "        self.state = None\n",
        "\n",
        "    def expanded(self):\n",
        "        return len(self.children) > 0\n",
        "\n",
        "    def value(self):\n",
        "        if self.visit_count == 0:\n",
        "            return 0\n",
        "        return self.value_sum / self.visit_count\n",
        "\n",
        "    def select_action(self, temperature):\n",
        "        \"\"\"\n",
        "        Select action according to the visit count distribution and the temperature.\n",
        "        \"\"\"\n",
        "        visit_counts = np.array([child.visit_count for child in self.children.values()])\n",
        "        actions = [action for action in self.children.keys()]\n",
        "        if temperature == 0:\n",
        "            action = actions[np.argmax(visit_counts)]\n",
        "        elif temperature == float(\"inf\"):\n",
        "            action = np.random.choice(actions)\n",
        "        else:\n",
        "            # See paper appendix Data Generation\n",
        "            visit_count_distribution = visit_counts ** (1 / temperature)\n",
        "            visit_count_distribution = visit_count_distribution / sum(visit_count_distribution)\n",
        "            action = np.random.choice(actions, p=visit_count_distribution)\n",
        "\n",
        "        return action\n",
        "\n",
        "    def select_child(self):\n",
        "        \"\"\"\n",
        "        Select the child with the highest UCB score.\n",
        "        \"\"\"\n",
        "        best_score = -np.inf\n",
        "        best_action = -1\n",
        "        best_child = None\n",
        "\n",
        "        for action, child in self.children.items():\n",
        "            score = ucb_score(self, child)\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_action = action\n",
        "                best_child = child\n",
        "\n",
        "        return best_action, best_child\n",
        "\n",
        "    def expand(self, state, to_play, action_probs):\n",
        "        \"\"\"\n",
        "        We expand a node and keep track of the prior policy probability given by neural network\n",
        "        \"\"\"\n",
        "        self.to_play = to_play\n",
        "        self.state = state\n",
        "        for a, prob in enumerate(action_probs):\n",
        "            if prob != 0:\n",
        "                self.children[a] = Node(prior=prob, to_play=self.to_play * -1)\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"\n",
        "        Debugger pretty print node info\n",
        "        \"\"\"\n",
        "        prior = \"{0:.2f}\".format(self.prior)\n",
        "        return \"{} Prior: {} Count: {} Value: {}\".format(self.state.__str__(), prior, self.visit_count, self.value())\n",
        "\n",
        "\n",
        "class MCTS:\n",
        "\n",
        "    def __init__(self, game, model, args):\n",
        "        self.game = game\n",
        "        self.model = model\n",
        "        self.args = args\n",
        "\n",
        "    def run(self, model, state, to_play):\n",
        "\n",
        "        root = Node(0, to_play)\n",
        "\n",
        "        # EXPAND root\n",
        "        action_probs, value = model.predict(state)\n",
        "        valid_moves = self.game.get_valid_moves(state)\n",
        "        action_probs = action_probs * valid_moves  # mask invalid moves\n",
        "        action_probs /= np.sum(action_probs)\n",
        "        root.expand(state, to_play, action_probs)\n",
        "\n",
        "        for _ in range(self.args['num_simulations']):\n",
        "            node = root\n",
        "            search_path = [node]\n",
        "\n",
        "            # SELECT\n",
        "            while node.expanded():\n",
        "                action, node = node.select_child()\n",
        "                search_path.append(node)\n",
        "\n",
        "            parent = search_path[-2]\n",
        "            state = parent.state\n",
        "            # Now we're at a leaf node and we would like to expand\n",
        "            # Players always play from their own perspective\n",
        "            next_state, _ = self.game.get_next_state(state, player=1, action=action)\n",
        "            # Get the board from the perspective of the other player\n",
        "            next_state = self.game.get_canonical_board(next_state, player=-1)\n",
        "\n",
        "            # The value of the new state from the perspective of the other player\n",
        "            value = self.game.get_reward_for_player(next_state, player=1)\n",
        "            if value is None:\n",
        "                # If the game has not ended:\n",
        "                # EXPAND\n",
        "                action_probs, value = model.predict(next_state)\n",
        "                valid_moves = self.game.get_valid_moves(next_state)\n",
        "                action_probs = action_probs * valid_moves  # mask invalid moves\n",
        "                action_probs /= np.sum(action_probs)\n",
        "                node.expand(next_state, parent.to_play * -1, action_probs)\n",
        "\n",
        "            self.backpropagate(search_path, value, parent.to_play * -1)\n",
        "\n",
        "        return root\n",
        "\n",
        "    def backpropagate(self, search_path, value, to_play):\n",
        "        \"\"\"\n",
        "        At the end of a simulation, we propagate the evaluation all the way up the tree\n",
        "        to the root.\n",
        "        \"\"\"\n",
        "        for node in reversed(search_path):\n",
        "            node.value_sum += value if node.to_play == to_play else -value\n",
        "            node.visit_count += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjGwnMH6cXs9"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJxO7RW0Tk8b"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdOdmt7jdKS_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "class TicTacToeModel(nn.Module):\n",
        "    def __init__(self, board_size, action_size):\n",
        "        super(TicTacToeModel, self).__init__()\n",
        "        self.board_height, self.board_width = board_size\n",
        "        self.size = board_size\n",
        "        self.action_size = 9\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        \n",
        "        self.fc1 = nn.Linear(in_features=64 * self.board_height * self.board_width, out_features=64)\n",
        "        self.bn4 = nn.BatchNorm1d(64)\n",
        "        self.fc2 = nn.Linear(in_features=64, out_features=64)\n",
        "        self.bn5 = nn.BatchNorm1d(64)\n",
        "\n",
        "        # Two heads on our network\n",
        "        self.action_head = nn.Linear(in_features=64, out_features=self.action_size)\n",
        "        self.value_head = nn.Linear(in_features=64, out_features=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 1, self.board_height, self.board_width)\n",
        "        x = self.bn1(self.conv1(x))\n",
        "        x = self.bn2(self.conv2(x))\n",
        "        #x = self.bn3(self.conv3(x))\n",
        "        x = x.view(-1, 64 * self.board_height * self.board_width)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.bn4(self.fc2(x))\n",
        "        x = torch.relu(x)\n",
        "\n",
        "        action_logits = self.action_head(x)\n",
        "        value_logit = self.value_head(x)\n",
        "\n",
        "        return torch.softmax(action_logits, dim=1), torch.tanh(value_logit)\n",
        "\n",
        "    def predict(self, board):\n",
        "        board = torch.FloatTensor(board.astype(np.float32))\n",
        "        board = board.view(3, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            pi, v = self.forward(board)\n",
        "\n",
        "        return pi.data.numpy()[0], v.data.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwFYIA3ocvpx"
      },
      "source": [
        "## Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9zmoMNNdKwp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from random import shuffle\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class Trainer:\n",
        "\n",
        "    def __init__(self, game, model, args):\n",
        "        self.game = game\n",
        "        self.model = model\n",
        "        self.args = args\n",
        "        self.mcts = MCTS(self.game, self.model, self.args)\n",
        "        self.losses_p = []\n",
        "        self.losses_v = []\n",
        "\n",
        "    def excecute_episode(self, reward_player_X, reward_player_O):\n",
        "\n",
        "        train_examples = []\n",
        "        current_player = 1\n",
        "        state = self.game.get_init_board()\n",
        "\n",
        "        while True:\n",
        "            canonical_board = self.game.get_canonical_board(state, current_player)\n",
        "\n",
        "            self.mcts = MCTS(self.game, self.model, self.args)\n",
        "            root = self.mcts.run(self.model, canonical_board, to_play=1)\n",
        "\n",
        "            action_probs = [0 for _ in range(self.game.get_action_size())]\n",
        "            for k, v in root.children.items():\n",
        "                action_probs[k] = v.visit_count\n",
        "\n",
        "            action_probs = action_probs / np.sum(action_probs)\n",
        "            train_examples.append((canonical_board, current_player, action_probs))\n",
        "\n",
        "            action = root.select_action(temperature=0)\n",
        "            state, current_player = self.game.get_next_state(state, current_player, action)\n",
        "            reward = self.game.get_reward_for_player(state, current_player)\n",
        "\n",
        "            if reward is not None:\n",
        "                ret = []\n",
        "                for hist_state, hist_current_player, hist_action_probs in train_examples:\n",
        "                    # [Board, currentPlayer, actionProbabilities, Reward]\n",
        "                    ret.append((hist_state, hist_action_probs, reward * ((-1) ** (hist_current_player != current_player))))\n",
        "                    if current_player == 1 :\n",
        "                      reward_player_X.append(reward)\n",
        "                    else:\n",
        "                      reward_player_O.append(reward)\n",
        "                return ret\n",
        "\n",
        "    def learn(self):\n",
        "      final_losses_pi = []\n",
        "      final_losses_v = []\n",
        "      reward_player_X = []\n",
        "      reward_player_O = []\n",
        "      \n",
        "      for i in range(1, self.args['numIters'] + 1):\n",
        "\n",
        "          print(\"{}/{}\".format(i, self.args['numIters']))\n",
        "\n",
        "          train_examples = []\n",
        "\n",
        "          for eps in range(self.args['numEps']):\n",
        "              iteration_train_examples = self.excecute_episode(reward_player_X, reward_player_O)\n",
        "              train_examples.extend(iteration_train_examples)\n",
        "\n",
        "          shuffle(train_examples)\n",
        "          self.train(train_examples, final_losses_pi, final_losses_v)\n",
        "          \n",
        "          filename = self.args['checkpoint_path']\n",
        "          self.save_checkpoint(folder=\".\", filename=filename)\n",
        "      return final_losses_pi, final_losses_v, reward_player_X, reward_player_O\n",
        "       \n",
        "\n",
        "    def train(self, examples, final_losses_pi, final_losses_v):\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=5e-4)\n",
        "        pi_losses = []\n",
        "        v_losses = []\n",
        "        best_out_pi = float('-inf')\n",
        "        best_model_params = None\n",
        "\n",
        "        for epoch in range(self.args['epochs']):\n",
        "            self.model.train()\n",
        "\n",
        "            batch_idx = 0\n",
        "\n",
        "            while batch_idx < int(len(examples) / self.args['batch_size']):\n",
        "                sample_ids = np.random.randint(len(examples), size=self.args['batch_size'])\n",
        "                boards, pis, vs = list(zip(*[examples[i] for i in sample_ids]))\n",
        "                boards = torch.FloatTensor(np.array(boards).astype(np.float64))\n",
        "                target_pis = torch.FloatTensor(np.array(pis))\n",
        "                target_vs = torch.FloatTensor(np.array(vs).astype(np.float64))\n",
        "\n",
        "                # predict\n",
        "                boards = boards.contiguous() \n",
        "                target_pis = target_pis.contiguous() \n",
        "                target_vs = target_vs.contiguous() \n",
        "\n",
        "                # compute output\n",
        "                out_pi, out_v = self.model(boards)\n",
        "\n",
        "\n",
        "                #compute the losses\n",
        "                l_pi = self.loss_pi(target_pis, out_pi)\n",
        "                l_v = self.loss_v(target_vs, out_v)\n",
        "                total_loss = l_pi + l_v\n",
        "\n",
        "                pi_losses.append(float(l_pi))\n",
        "                v_losses.append(float(l_v))\n",
        "\n",
        "                if total_loss < best_out_pi :\n",
        "                  best_out_pi = out_pi\n",
        "                  best_model_params = [p.detach().numpy() for p in self.model.parameters()]\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                total_loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                batch_idx += 1\n",
        "\n",
        "            print()\n",
        "            print(\"Policy Loss\", np.mean(pi_losses))\n",
        "            final_losses_pi.append(np.mean(pi_losses))\n",
        "            print(\"Value Loss\", np.mean(v_losses))\n",
        "            final_losses_v.append(np.mean(v_losses))\n",
        "            print(\"Examples:\")\n",
        "            print(out_pi[0].detach())\n",
        "            print(target_pis[0])\n",
        "        # save best_model_params after training\n",
        "        np.save(\"best_model_params.npy\", best_model_params)\n",
        "        print(\"the best pi is :\", best_out_pi)\n",
        "        \n",
        "    def loss_pi(self, targets, outputs):\n",
        "        loss = -(targets * torch.log(outputs)).sum(dim=1)\n",
        "        return loss.mean()\n",
        "\n",
        "    def loss_v(self, targets, outputs):\n",
        "        loss = torch.sum((targets-outputs.view(-1))**2)/targets.size()[0]\n",
        "        return loss\n",
        "\n",
        "    def save_checkpoint(self, folder, filename):\n",
        "        if not os.path.exists(folder):\n",
        "            os.mkdir(folder)\n",
        "\n",
        "        filepath = os.path.join(folder, filename)\n",
        "        torch.save({'state_dict': self.model.state_dict(),}, filepath)\n",
        "    \n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMzJhTcMiw5Q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zC-Bdzs5dG-c"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f5J7vjodIwh",
        "outputId": "c1e69d02-f0d1-4b8f-ec1e-6bac0e09e050"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-204b893cafc0>:16: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  b = np.zeros((self.rows, self.columns), dtype=np.int)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Policy Loss 2.1670289039611816\n",
            "Value Loss 0.29208948612213137\n",
            "Examples:\n",
            "tensor([0.1313, 0.2576, 0.0948, 0.0456, 0.0974, 0.0673, 0.0776, 0.0919, 0.1365])\n",
            "tensor([0.0000, 0.9200, 0.0000, 0.0400, 0.0000, 0.0400, 0.0000, 0.0000, 0.0000])\n",
            "\n",
            "Policy Loss 2.0470427870750427\n",
            "Value Loss 0.21709052100777626\n",
            "Examples:\n",
            "tensor([0.2100, 0.0803, 0.1443, 0.0491, 0.2128, 0.0664, 0.1191, 0.0425, 0.0755])\n",
            "tensor([0.3800, 0.2000, 0.0000, 0.2200, 0.0000, 0.2000, 0.0000, 0.0000, 0.0000])\n",
            "the best pi is : -inf\n",
            "2/50\n",
            "\n",
            "Policy Loss 2.013768332345145\n",
            "Value Loss 0.07532383581357342\n",
            "Examples:\n",
            "tensor([0.1039, 0.0932, 0.1321, 0.1660, 0.2130, 0.0578, 0.0885, 0.0621, 0.0834])\n",
            "tensor([0.0000, 0.0000, 0.0000, 0.9200, 0.0000, 0.0800, 0.0000, 0.0000, 0.0000])\n",
            "\n",
            "Policy Loss 1.8754300219672067\n",
            "Value Loss 0.04104170148327414\n",
            "Examples:\n",
            "tensor([0.1201, 0.0907, 0.1130, 0.0879, 0.1488, 0.0802, 0.1156, 0.1223, 0.1213])\n",
            "tensor([0.1000, 0.1000, 0.0800, 0.1000, 0.1800, 0.1000, 0.1000, 0.2400, 0.0000])\n",
            "the best pi is : -inf\n",
            "3/50\n",
            "\n",
            "Policy Loss 2.0090697526931764\n",
            "Value Loss 0.4169056057929993\n",
            "Examples:\n",
            "tensor([0.1477, 0.1126, 0.1057, 0.0847, 0.1075, 0.0899, 0.1533, 0.0762, 0.1223])\n",
            "tensor([0.1200, 0.1000, 0.0800, 0.2000, 0.0200, 0.0800, 0.0800, 0.0000, 0.3200])\n",
            "\n",
            "Policy Loss 1.9611412763595581\n",
            "Value Loss 0.27382868677377703\n",
            "Examples:\n",
            "tensor([0.1207, 0.1784, 0.1687, 0.0521, 0.0812, 0.1622, 0.1086, 0.0552, 0.0727])\n",
            "tensor([0.0000, 0.3600, 0.2200, 0.0000, 0.0000, 0.2600, 0.1600, 0.0000, 0.0000])\n",
            "the best pi is : -inf\n",
            "4/50\n",
            "\n",
            "Policy Loss 2.2166746854782104\n",
            "Value Loss 1.2448821763197582\n",
            "Examples:\n",
            "tensor([0.0914, 0.1340, 0.1563, 0.1360, 0.0978, 0.0767, 0.1624, 0.0631, 0.0823])\n",
            "tensor([0.0600, 0.0000, 0.0600, 0.0000, 0.7600, 0.0400, 0.0400, 0.0000, 0.0400])\n",
            "\n",
            "Policy Loss 2.0983787377675376\n",
            "Value Loss 0.7582409953077635\n",
            "Examples:\n",
            "tensor([0.0800, 0.1053, 0.1697, 0.0458, 0.1998, 0.1483, 0.1392, 0.0214, 0.0904])\n",
            "tensor([0.0000, 0.0000, 0.2800, 0.0000, 0.0000, 0.0000, 0.3600, 0.0000, 0.3600])\n",
            "the best pi is : -inf\n",
            "5/50\n",
            "\n",
            "Policy Loss 2.0603535005024503\n",
            "Value Loss 0.12546649522015027\n",
            "Examples:\n",
            "tensor([0.1077, 0.1085, 0.1203, 0.0627, 0.1908, 0.0374, 0.0725, 0.0924, 0.2078])\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
            "\n",
            "Policy Loss 1.883061477116176\n",
            "Value Loss 0.07319102196821145\n",
            "Examples:\n",
            "tensor([0.1007, 0.0660, 0.0981, 0.1133, 0.2677, 0.0649, 0.1090, 0.0558, 0.1244])\n",
            "tensor([0.1000, 0.0400, 0.1000, 0.0000, 0.5000, 0.1000, 0.1000, 0.0200, 0.0400])\n",
            "the best pi is : -inf\n",
            "6/50\n",
            "\n",
            "Policy Loss 1.8747262273515974\n",
            "Value Loss 0.025222804663436755\n",
            "Examples:\n",
            "tensor([0.0790, 0.0751, 0.1662, 0.1843, 0.1340, 0.1019, 0.0807, 0.1023, 0.0766])\n",
            "tensor([0.0600, 0.0200, 0.0600, 0.7400, 0.0000, 0.0400, 0.0400, 0.0200, 0.0200])\n",
            "\n",
            "Policy Loss 1.7225849032402039\n",
            "Value Loss 0.017074760448719774\n",
            "Examples:\n",
            "tensor([0.1201, 0.0606, 0.0675, 0.0564, 0.1333, 0.1202, 0.2453, 0.0857, 0.1109])\n",
            "tensor([0.0000, 0.0200, 0.0600, 0.0000, 0.0000, 0.2000, 0.4400, 0.2800, 0.0000])\n",
            "the best pi is : -inf\n",
            "7/50\n",
            "\n",
            "Policy Loss 2.0109325647354126\n",
            "Value Loss 0.530942302942276\n",
            "Examples:\n",
            "tensor([0.0624, 0.1276, 0.1907, 0.1814, 0.0976, 0.1792, 0.0472, 0.0512, 0.0627])\n",
            "tensor([0.0200, 0.0200, 0.2400, 0.6400, 0.0000, 0.0200, 0.0200, 0.0200, 0.0200])\n",
            "\n",
            "Policy Loss 1.9425434112548827\n",
            "Value Loss 0.3307345904409885\n",
            "Examples:\n",
            "tensor([0.1281, 0.0745, 0.1291, 0.0399, 0.1259, 0.1146, 0.1450, 0.0557, 0.1870])\n",
            "tensor([0.0000, 0.1600, 0.0000, 0.0000, 0.0000, 0.0800, 0.0000, 0.0800, 0.6800])\n",
            "the best pi is : -inf\n",
            "8/50\n",
            "\n",
            "Policy Loss 1.75468567439488\n",
            "Value Loss 0.12047496151977352\n",
            "Examples:\n",
            "tensor([0.1546, 0.1703, 0.1211, 0.0396, 0.0826, 0.1524, 0.1185, 0.0448, 0.1160])\n",
            "tensor([0.2400, 0.2400, 0.0000, 0.0000, 0.0000, 0.2000, 0.2400, 0.0000, 0.0800])\n",
            "\n",
            "Policy Loss 1.6412709525653295\n",
            "Value Loss 0.06528283565837358\n",
            "Examples:\n",
            "tensor([0.0528, 0.1145, 0.1085, 0.2872, 0.1002, 0.1189, 0.0680, 0.0740, 0.0757])\n",
            "tensor([0.0000, 0.1800, 0.0000, 0.7800, 0.0000, 0.0000, 0.0200, 0.0200, 0.0000])\n",
            "the best pi is : -inf\n",
            "9/50\n",
            "\n",
            "Policy Loss 1.7419500010354179\n",
            "Value Loss 0.008221803565642663\n",
            "Examples:\n",
            "tensor([0.0891, 0.2153, 0.0681, 0.0241, 0.0693, 0.3192, 0.0854, 0.0564, 0.0731])\n",
            "tensor([0.1400, 0.2400, 0.0000, 0.0200, 0.0000, 0.4200, 0.0800, 0.0200, 0.0800])\n",
            "\n",
            "Policy Loss 1.6442347935267858\n",
            "Value Loss 0.005218730894349781\n",
            "Examples:\n",
            "tensor([0.0979, 0.0437, 0.0614, 0.0300, 0.0760, 0.0547, 0.0582, 0.4935, 0.0846])\n",
            "tensor([0.1200, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0400, 0.7400, 0.1000])\n",
            "the best pi is : -inf\n",
            "10/50\n",
            "\n",
            "Policy Loss 1.4147676910672868\n",
            "Value Loss 0.0022568690058376107\n",
            "Examples:\n",
            "tensor([0.0988, 0.1677, 0.0654, 0.0325, 0.0767, 0.3012, 0.0925, 0.0732, 0.0919])\n",
            "tensor([0.1600, 0.1400, 0.0000, 0.0200, 0.0000, 0.5600, 0.0600, 0.0200, 0.0400])\n",
            "\n",
            "Policy Loss 1.3602591412408012\n",
            "Value Loss 0.0029919621301814914\n",
            "Examples:\n",
            "tensor([0.0951, 0.3274, 0.0522, 0.0277, 0.0499, 0.0941, 0.2145, 0.0327, 0.1063])\n",
            "tensor([0.1400, 0.4200, 0.0000, 0.0000, 0.0000, 0.0000, 0.2800, 0.0200, 0.1400])\n",
            "the best pi is : -inf\n",
            "11/50\n",
            "\n",
            "Policy Loss 1.67951238155365\n",
            "Value Loss 0.43497365961472195\n",
            "Examples:\n",
            "tensor([0.2576, 0.0522, 0.0563, 0.0428, 0.0604, 0.0695, 0.0832, 0.3088, 0.0693])\n",
            "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8800, 0.1200, 0.0000])\n",
            "\n",
            "Policy Loss 1.643142819404602\n",
            "Value Loss 0.2738619310160478\n",
            "Examples:\n",
            "tensor([0.0514, 0.0283, 0.0695, 0.0455, 0.5401, 0.0477, 0.0578, 0.0541, 0.1055])\n",
            "tensor([0.0800, 0.0200, 0.3000, 0.0600, 0.3200, 0.1000, 0.0200, 0.0400, 0.0600])\n",
            "the best pi is : -inf\n",
            "12/50\n",
            "\n",
            "Policy Loss 2.347833251953125\n",
            "Value Loss 0.6554060041904449\n",
            "Examples:\n",
            "tensor([0.1247, 0.1203, 0.0899, 0.1037, 0.1347, 0.0773, 0.1103, 0.1131, 0.1260])\n",
            "tensor([0.1400, 0.0000, 0.0000, 0.5000, 0.0200, 0.0400, 0.0200, 0.2000, 0.0800])\n",
            "\n",
            "Policy Loss 2.2560992002487184\n",
            "Value Loss 0.4645648494362831\n",
            "Examples:\n",
            "tensor([0.1243, 0.1203, 0.1365, 0.0930, 0.0822, 0.0872, 0.1203, 0.0767, 0.1596])\n",
            "tensor([0.0600, 0.0000, 0.0000, 0.0000, 0.0200, 0.0800, 0.0600, 0.6600, 0.1200])\n",
            "the best pi is : -inf\n",
            "13/50\n",
            "\n",
            "Policy Loss 2.2071829636891684\n",
            "Value Loss 0.3226191848516464\n",
            "Examples:\n",
            "tensor([0.2892, 0.0556, 0.1242, 0.0102, 0.0352, 0.1346, 0.1886, 0.0416, 0.1208])\n",
            "tensor([0.2800, 0.0400, 0.0000, 0.0000, 0.1800, 0.2200, 0.2000, 0.0800, 0.0000])\n",
            "\n",
            "Policy Loss 2.1392374436060586\n",
            "Value Loss 0.21020491048693657\n",
            "Examples:\n",
            "tensor([0.1821, 0.1153, 0.0729, 0.0921, 0.0768, 0.1096, 0.1259, 0.1239, 0.1014])\n",
            "tensor([0.1600, 0.1400, 0.0000, 0.0000, 0.2000, 0.0400, 0.1400, 0.0600, 0.2600])\n",
            "the best pi is : -inf\n",
            "14/50\n",
            "\n",
            "Policy Loss 2.304496560777937\n",
            "Value Loss 0.13060846046677657\n",
            "Examples:\n",
            "tensor([0.0851, 0.1316, 0.2064, 0.1000, 0.0608, 0.1426, 0.0823, 0.1248, 0.0665])\n",
            "tensor([0.0000, 0.1200, 0.0000, 0.0000, 0.0000, 0.6600, 0.0000, 0.0600, 0.1600])\n",
            "\n",
            "Policy Loss 2.0944927505084445\n",
            "Value Loss 0.08258904104254075\n",
            "Examples:\n",
            "tensor([0.1417, 0.1088, 0.1632, 0.0841, 0.1217, 0.0518, 0.1689, 0.0552, 0.1046])\n",
            "tensor([0.4800, 0.0800, 0.0000, 0.1200, 0.1000, 0.0400, 0.0600, 0.0400, 0.0800])\n",
            "the best pi is : -inf\n",
            "15/50\n",
            "\n",
            "Policy Loss 2.150786781311035\n",
            "Value Loss 0.3629628896713257\n",
            "Examples:\n",
            "tensor([0.1481, 0.1063, 0.0772, 0.0426, 0.1362, 0.0770, 0.1598, 0.0851, 0.1677])\n",
            "tensor([0.0000, 0.0400, 0.0000, 0.0400, 0.6800, 0.1200, 0.0000, 0.0600, 0.0600])\n",
            "\n",
            "Policy Loss 2.0080728411674498\n",
            "Value Loss 0.23123605772852898\n",
            "Examples:\n",
            "tensor([0.1052, 0.0341, 0.2320, 0.1246, 0.1980, 0.0692, 0.0921, 0.0674, 0.0775])\n",
            "tensor([0.3200, 0.0200, 0.4600, 0.0400, 0.0400, 0.0200, 0.0200, 0.0200, 0.0600])\n",
            "the best pi is : -inf\n",
            "16/50\n",
            "\n",
            "Policy Loss 2.036647001902262\n",
            "Value Loss 0.07740334421396255\n",
            "Examples:\n",
            "tensor([0.1176, 0.0929, 0.0649, 0.1056, 0.0672, 0.0854, 0.2084, 0.0906, 0.1675])\n",
            "tensor([0.0000, 0.0600, 0.0000, 0.1800, 0.1400, 0.0000, 0.3400, 0.0000, 0.2800])\n",
            "\n",
            "Policy Loss 1.9473764499028523\n",
            "Value Loss 0.06450786006947358\n",
            "Examples:\n",
            "tensor([0.2214, 0.0856, 0.0928, 0.0652, 0.0838, 0.1335, 0.0918, 0.0851, 0.1408])\n",
            "tensor([0.2000, 0.0400, 0.0000, 0.1600, 0.0800, 0.1200, 0.1000, 0.1800, 0.1200])\n",
            "the best pi is : -inf\n",
            "17/50\n",
            "\n",
            "Policy Loss 1.8506244818369548\n",
            "Value Loss 0.06012128541866938\n",
            "Examples:\n",
            "tensor([0.1095, 0.1400, 0.0619, 0.0440, 0.0547, 0.1513, 0.0742, 0.1426, 0.2219])\n",
            "tensor([0.0000, 0.0600, 0.0000, 0.2800, 0.0800, 0.1200, 0.0000, 0.3200, 0.1400])\n",
            "\n",
            "Policy Loss 1.7911184430122375\n",
            "Value Loss 0.04718411092956861\n",
            "Examples:\n",
            "tensor([0.0273, 0.0215, 0.6031, 0.0854, 0.0768, 0.0337, 0.0541, 0.0504, 0.0479])\n",
            "tensor([0.0200, 0.0000, 0.9400, 0.0200, 0.0200, 0.0000, 0.0000, 0.0000, 0.0000])\n",
            "the best pi is : -inf\n",
            "18/50\n",
            "\n",
            "Policy Loss 1.7080399513244628\n",
            "Value Loss 0.030108302459120752\n",
            "Examples:\n",
            "tensor([0.1239, 0.0547, 0.0413, 0.0316, 0.1525, 0.0980, 0.0583, 0.3253, 0.1144])\n",
            "tensor([0.0000, 0.1000, 0.0000, 0.0000, 0.3600, 0.1200, 0.0000, 0.4200, 0.0000])\n",
            "\n",
            "Policy Loss 1.647609293460846\n",
            "Value Loss 0.02491321451961994\n",
            "Examples:\n",
            "tensor([0.0821, 0.0779, 0.0592, 0.0461, 0.0335, 0.0283, 0.4104, 0.0507, 0.2118])\n",
            "tensor([0.0000, 0.0400, 0.0000, 0.0000, 0.0000, 0.0000, 0.6600, 0.0000, 0.3000])\n",
            "the best pi is : -inf\n",
            "19/50\n",
            "\n",
            "Policy Loss 1.5915390650431316\n",
            "Value Loss 0.020398182794451714\n",
            "Examples:\n",
            "tensor([0.0238, 0.0302, 0.6159, 0.0772, 0.0664, 0.0409, 0.0513, 0.0485, 0.0459])\n",
            "tensor([0.0200, 0.0000, 0.9800, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])\n",
            "\n",
            "Policy Loss 1.525705913702647\n",
            "Value Loss 0.02035064436495304\n",
            "Examples:\n",
            "tensor([0.2160, 0.0711, 0.0615, 0.0921, 0.0970, 0.1744, 0.0749, 0.0893, 0.1237])\n",
            "tensor([0.2000, 0.0400, 0.0000, 0.1800, 0.0800, 0.1600, 0.1600, 0.0600, 0.1200])\n",
            "the best pi is : -inf\n",
            "20/50\n",
            "\n",
            "Policy Loss 1.9469725608825683\n",
            "Value Loss 0.038750645518302915\n",
            "Examples:\n",
            "tensor([0.1087, 0.0823, 0.0522, 0.0338, 0.0886, 0.0735, 0.3441, 0.0671, 0.1498])\n",
            "tensor([0.2600, 0.0000, 0.0000, 0.0000, 0.1800, 0.0200, 0.5200, 0.0200, 0.0000])\n",
            "\n",
            "Policy Loss 1.8076165914535522\n",
            "Value Loss 0.027275884244590997\n",
            "Examples:\n",
            "tensor([0.1104, 0.0692, 0.0407, 0.0331, 0.0495, 0.0475, 0.4612, 0.0547, 0.1336])\n",
            "tensor([0.2600, 0.0000, 0.0000, 0.0000, 0.1800, 0.0200, 0.5200, 0.0200, 0.0000])\n",
            "the best pi is : -inf\n",
            "21/50\n",
            "\n",
            "Policy Loss 1.5446868340174358\n",
            "Value Loss 0.022496537615855534\n",
            "Examples:\n",
            "tensor([0.1273, 0.0415, 0.0574, 0.0716, 0.2182, 0.1261, 0.0865, 0.1022, 0.1692])\n",
            "tensor([0.1200, 0.0400, 0.0000, 0.0800, 0.2400, 0.1200, 0.1800, 0.0800, 0.1400])\n",
            "\n",
            "Policy Loss 1.4810895721117656\n",
            "Value Loss 0.0247759601722161\n",
            "Examples:\n",
            "tensor([0.0605, 0.0770, 0.0269, 0.0981, 0.0951, 0.4653, 0.0443, 0.0433, 0.0896])\n",
            "tensor([0.0000, 0.0800, 0.0000, 0.2000, 0.0000, 0.5800, 0.0600, 0.0200, 0.0600])\n",
            "the best pi is : -inf\n",
            "22/50\n",
            "\n",
            "Policy Loss 1.4205189148585002\n",
            "Value Loss 0.017571111520131428\n",
            "Examples:\n",
            "tensor([0.1023, 0.0337, 0.0551, 0.0653, 0.2623, 0.1104, 0.1077, 0.1170, 0.1461])\n",
            "tensor([0.1000, 0.0200, 0.0000, 0.0400, 0.4000, 0.1000, 0.1600, 0.0600, 0.1200])\n",
            "\n",
            "Policy Loss 1.3423066933949788\n",
            "Value Loss 0.019788425415754318\n",
            "Examples:\n",
            "tensor([0.0707, 0.0960, 0.0388, 0.1109, 0.0855, 0.3309, 0.0896, 0.0550, 0.1225])\n",
            "tensor([0.0000, 0.1000, 0.0000, 0.1000, 0.0000, 0.4400, 0.2000, 0.0400, 0.1200])\n",
            "the best pi is : -inf\n",
            "23/50\n",
            "\n",
            "Policy Loss 1.184244950612386\n",
            "Value Loss 0.0180340309937795\n",
            "Examples:\n",
            "tensor([0.0602, 0.5338, 0.0216, 0.0467, 0.0387, 0.0778, 0.0491, 0.0954, 0.0767])\n",
            "tensor([0.0000, 0.9200, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0800, 0.0000])\n",
            "\n",
            "Policy Loss 1.165608028570811\n",
            "Value Loss 0.017623917510112126\n",
            "Examples:\n",
            "tensor([0.0716, 0.0235, 0.0411, 0.0582, 0.3785, 0.1287, 0.1074, 0.0732, 0.1177])\n",
            "tensor([0.0800, 0.0200, 0.0000, 0.0400, 0.4000, 0.1600, 0.1400, 0.0600, 0.1000])\n",
            "the best pi is : -inf\n",
            "24/50\n",
            "\n",
            "Policy Loss 1.2127461830774944\n",
            "Value Loss 0.023027171691258747\n",
            "Examples:\n",
            "tensor([0.0484, 0.1233, 0.0456, 0.1077, 0.0343, 0.2289, 0.2256, 0.0276, 0.1587])\n",
            "tensor([0.0000, 0.1000, 0.0000, 0.0800, 0.0000, 0.1600, 0.5400, 0.0200, 0.1000])\n",
            "\n",
            "Policy Loss 1.1604495247205098\n",
            "Value Loss 0.02174311379591624\n",
            "Examples:\n",
            "tensor([0.6831, 0.0593, 0.0147, 0.0212, 0.0205, 0.0385, 0.0607, 0.0441, 0.0581])\n",
            "tensor([0.9800, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0200, 0.0000, 0.0000])\n",
            "the best pi is : -inf\n",
            "25/50\n",
            "\n",
            "Policy Loss 1.0022038618723552\n",
            "Value Loss 0.018403319641947746\n",
            "Examples:\n",
            "tensor([0.0486, 0.1148, 0.0529, 0.1252, 0.0557, 0.1910, 0.2283, 0.0389, 0.1446])\n",
            "tensor([0.0000, 0.1800, 0.0000, 0.1200, 0.0000, 0.1400, 0.4000, 0.0200, 0.1400])\n",
            "\n",
            "Policy Loss 0.9458383917808533\n",
            "Value Loss 0.02049940824508667\n",
            "Examples:\n",
            "tensor([0.0530, 0.6391, 0.0232, 0.0252, 0.0232, 0.0560, 0.0491, 0.0527, 0.0784])\n",
            "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
            "the best pi is : -inf\n",
            "26/50\n",
            "\n",
            "Policy Loss 1.0208851496378581\n",
            "Value Loss 0.015294109471142292\n",
            "Examples:\n",
            "tensor([0.7829, 0.0411, 0.0091, 0.0142, 0.0131, 0.0271, 0.0423, 0.0334, 0.0369])\n",
            "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "\n",
            "Policy Loss 0.973131020863851\n",
            "Value Loss 0.013238109493007263\n",
            "Examples:\n",
            "tensor([0.0283, 0.7408, 0.0203, 0.0164, 0.0154, 0.0429, 0.0336, 0.0425, 0.0599])\n",
            "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
            "the best pi is : -inf\n",
            "27/50\n",
            "\n",
            "Policy Loss 0.8955447872479757\n",
            "Value Loss 0.011727061122655869\n",
            "Examples:\n",
            "tensor([0.0308, 0.1620, 0.0347, 0.1047, 0.0437, 0.1899, 0.2672, 0.0310, 0.1361])\n",
            "tensor([0.0000, 0.2600, 0.0000, 0.1000, 0.0000, 0.1600, 0.3400, 0.0200, 0.1200])\n",
            "\n",
            "Policy Loss 0.9064487914244334\n",
            "Value Loss 0.010210590437054634\n",
            "Examples:\n",
            "tensor([0.0279, 0.1725, 0.0325, 0.1024, 0.0407, 0.1828, 0.2855, 0.0266, 0.1292])\n",
            "tensor([0.0000, 0.2600, 0.0000, 0.1000, 0.0000, 0.1600, 0.3400, 0.0200, 0.1200])\n",
            "the best pi is : -inf\n",
            "28/50\n",
            "\n",
            "Policy Loss 1.513070753642491\n",
            "Value Loss 0.4768955409526825\n",
            "Examples:\n",
            "tensor([0.0688, 0.1392, 0.0413, 0.0702, 0.0329, 0.1450, 0.3852, 0.0162, 0.1012])\n",
            "tensor([0.0000, 0.0000, 0.0000, 0.0200, 0.0000, 0.0200, 0.9400, 0.0000, 0.0200])\n",
            "\n",
            "Policy Loss 1.390048929623195\n",
            "Value Loss 0.3280281972672258\n",
            "Examples:\n",
            "tensor([0.0592, 0.2792, 0.0658, 0.0510, 0.0267, 0.0751, 0.3229, 0.0244, 0.0958])\n",
            "tensor([0.0000, 0.5400, 0.0000, 0.0600, 0.0000, 0.1000, 0.2000, 0.0200, 0.0800])\n",
            "the best pi is : -inf\n",
            "29/50\n",
            "\n",
            "Policy Loss 1.1179055656705583\n",
            "Value Loss 0.0704503282904625\n",
            "Examples:\n",
            "tensor([0.0292, 0.4295, 0.0235, 0.0360, 0.0246, 0.1341, 0.2117, 0.0288, 0.0826])\n",
            "tensor([0.0000, 0.7200, 0.0000, 0.0200, 0.0000, 0.0200, 0.1400, 0.0200, 0.0800])\n",
            "\n",
            "Policy Loss 1.0681076688425881\n",
            "Value Loss 0.04735198271061693\n",
            "Examples:\n",
            "tensor([0.0665, 0.0574, 0.0825, 0.0839, 0.3377, 0.1175, 0.0555, 0.1158, 0.0832])\n",
            "tensor([0.0200, 0.0200, 0.0000, 0.0600, 0.4800, 0.2800, 0.0400, 0.0800, 0.0200])\n",
            "the best pi is : -inf\n",
            "30/50\n",
            "\n",
            "Policy Loss 0.9148686272757394\n",
            "Value Loss 0.01658833898337824\n",
            "Examples:\n",
            "tensor([0.0554, 0.0523, 0.0620, 0.0710, 0.3945, 0.1424, 0.0469, 0.0994, 0.0761])\n",
            "tensor([0.0200, 0.0200, 0.0000, 0.0200, 0.6200, 0.2600, 0.0200, 0.0200, 0.0200])\n",
            "\n",
            "Policy Loss 0.889344300542559\n",
            "Value Loss 0.011065254852707897\n",
            "Examples:\n",
            "tensor([0.0830, 0.0804, 0.0768, 0.1131, 0.0545, 0.1368, 0.0634, 0.2674, 0.1247])\n",
            "tensor([0.0000, 0.0000, 0.0000, 0.1600, 0.0000, 0.2400, 0.0200, 0.3800, 0.2000])\n",
            "the best pi is : -inf\n",
            "31/50\n",
            "\n",
            "Policy Loss 1.2738725543022156\n",
            "Value Loss 0.9136093854904175\n",
            "Examples:\n",
            "tensor([0.9301, 0.0041, 0.0016, 0.0059, 0.0017, 0.0092, 0.0134, 0.0076, 0.0264])\n",
            "tensor([0.9800, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0200])\n",
            "\n",
            "Policy Loss 1.2984554022550583\n",
            "Value Loss 0.5755642279982567\n",
            "Examples:\n",
            "tensor([0.0501, 0.1311, 0.0190, 0.0787, 0.0439, 0.2847, 0.2783, 0.0356, 0.0787])\n",
            "tensor([0.0000, 0.0000, 0.0000, 0.0600, 0.0000, 0.1600, 0.1200, 0.6600, 0.0000])\n",
            "the best pi is : -inf\n",
            "32/50\n",
            "\n",
            "Policy Loss 1.9685086727142334\n",
            "Value Loss 0.15970574766397477\n",
            "Examples:\n",
            "tensor([0.0181, 0.6055, 0.0074, 0.0549, 0.0097, 0.1991, 0.0363, 0.0249, 0.0441])\n",
            "tensor([0.0000, 0.8400, 0.0000, 0.1600, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])\n",
            "\n",
            "Policy Loss 1.7460406064987182\n",
            "Value Loss 0.1068597862496972\n",
            "Examples:\n",
            "tensor([0.0551, 0.1497, 0.0402, 0.1309, 0.0706, 0.1809, 0.0527, 0.2524, 0.0674])\n",
            "tensor([0.0000, 0.0000, 0.0000, 0.0400, 0.0000, 0.0000, 0.0000, 0.8600, 0.1000])\n",
            "the best pi is : -inf\n",
            "33/50\n",
            "\n",
            "Policy Loss 1.9923192501068114\n",
            "Value Loss 0.06247093379497528\n",
            "Examples:\n",
            "tensor([0.1831, 0.0293, 0.0559, 0.1169, 0.0970, 0.1558, 0.0752, 0.0715, 0.2154])\n",
            "tensor([0.1600, 0.0400, 0.0400, 0.2200, 0.0000, 0.2000, 0.0600, 0.0600, 0.2200])\n",
            "\n",
            "Policy Loss 1.8072551608085632\n",
            "Value Loss 0.047613319009542465\n",
            "Examples:\n",
            "tensor([0.1069, 0.0584, 0.0844, 0.1563, 0.1006, 0.1496, 0.0940, 0.0799, 0.1698])\n",
            "tensor([0.1600, 0.0400, 0.0400, 0.2200, 0.0000, 0.2000, 0.0600, 0.0600, 0.2200])\n",
            "the best pi is : -inf\n",
            "34/50\n",
            "\n",
            "Policy Loss 1.5438976049423219\n",
            "Value Loss 0.04463207013905048\n",
            "Examples:\n",
            "tensor([0.0182, 0.0211, 0.2370, 0.0717, 0.4734, 0.0642, 0.0268, 0.0359, 0.0517])\n",
            "tensor([0.0200, 0.0000, 0.0200, 0.0000, 0.9600, 0.0000, 0.0000, 0.0000, 0.0000])\n",
            "\n",
            "Policy Loss 1.4698480725288392\n",
            "Value Loss 0.038473686948418614\n",
            "Examples:\n",
            "tensor([0.0089, 0.0103, 0.1944, 0.0478, 0.6382, 0.0433, 0.0131, 0.0151, 0.0289])\n",
            "tensor([0.0200, 0.0000, 0.0200, 0.0000, 0.9600, 0.0000, 0.0000, 0.0000, 0.0000])\n",
            "the best pi is : -inf\n",
            "35/50\n",
            "\n",
            "Policy Loss 1.5361930847167968\n",
            "Value Loss 0.032300544902682304\n",
            "Examples:\n",
            "tensor([0.4516, 0.0981, 0.0372, 0.0468, 0.0087, 0.0671, 0.1156, 0.0381, 0.1368])\n",
            "tensor([0.5600, 0.0000, 0.0000, 0.1200, 0.0000, 0.0000, 0.0000, 0.0000, 0.3200])\n",
            "\n",
            "Policy Loss 1.4019761204719543\n",
            "Value Loss 0.03186326213181019\n",
            "Examples:\n",
            "tensor([0.1014, 0.0434, 0.0690, 0.1247, 0.0704, 0.1312, 0.1106, 0.2008, 0.1484])\n",
            "tensor([0.1200, 0.0000, 0.1000, 0.1400, 0.0000, 0.0000, 0.1200, 0.3600, 0.1600])\n",
            "the best pi is : -inf\n",
            "36/50\n",
            "\n",
            "Policy Loss 1.3728638887405396\n",
            "Value Loss 0.019452409197886784\n",
            "Examples:\n",
            "tensor([0.2172, 0.0691, 0.2093, 0.0666, 0.0274, 0.1084, 0.0408, 0.1644, 0.0968])\n",
            "tensor([0.1200, 0.0000, 0.1600, 0.0000, 0.0000, 0.0000, 0.0000, 0.7200, 0.0000])\n",
            "\n",
            "Policy Loss 1.3317479888598125\n",
            "Value Loss 0.015949669294059277\n",
            "Examples:\n",
            "tensor([0.0477, 0.7548, 0.0050, 0.0263, 0.0039, 0.0649, 0.0542, 0.0156, 0.0276])\n",
            "tensor([0.0400, 0.9600, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])\n",
            "the best pi is : -inf\n",
            "37/50\n",
            "\n",
            "Policy Loss 1.4089921474456788\n",
            "Value Loss 0.015694746188819407\n",
            "Examples:\n",
            "tensor([0.0441, 0.6283, 0.0077, 0.0374, 0.0074, 0.1631, 0.0408, 0.0231, 0.0480])\n",
            "tensor([0.0800, 0.6800, 0.0000, 0.0000, 0.0000, 0.2400, 0.0000, 0.0000, 0.0000])\n",
            "\n",
            "Policy Loss 1.3768638491630554\n",
            "Value Loss 0.013747912924736738\n",
            "Examples:\n",
            "tensor([0.1069, 0.0366, 0.0362, 0.0726, 0.0627, 0.1339, 0.2079, 0.2027, 0.1405])\n",
            "tensor([0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1600, 0.3400, 0.3000, 0.0000])\n",
            "the best pi is : -inf\n",
            "38/50\n",
            "\n",
            "Policy Loss 1.1488590002059937\n",
            "Value Loss 0.014160212874412537\n",
            "Examples:\n",
            "tensor([0.0782, 0.0420, 0.0568, 0.0773, 0.0370, 0.1727, 0.1798, 0.1734, 0.1828])\n",
            "tensor([0.1000, 0.0000, 0.0400, 0.0000, 0.0000, 0.2400, 0.1800, 0.2000, 0.2400])\n",
            "\n",
            "Policy Loss 1.124501645565033\n",
            "Value Loss 0.01303177634254098\n",
            "Examples:\n",
            "tensor([0.0595, 0.0281, 0.0105, 0.0345, 0.0189, 0.0562, 0.1902, 0.0530, 0.5490])\n",
            "tensor([0.0000, 0.0000, 0.0200, 0.0000, 0.0000, 0.0000, 0.2800, 0.0600, 0.6400])\n",
            "the best pi is : -inf\n",
            "39/50\n",
            "\n",
            "Policy Loss 1.2506672938664753\n",
            "Value Loss 0.010111562597254911\n",
            "Examples:\n",
            "tensor([0.0510, 0.8284, 0.0027, 0.0098, 0.0020, 0.0576, 0.0248, 0.0098, 0.0139])\n",
            "tensor([0.0400, 0.9600, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])\n",
            "\n",
            "Policy Loss 1.221550186475118\n",
            "Value Loss 0.010618800142159065\n",
            "Examples:\n",
            "tensor([0.0838, 0.0167, 0.0290, 0.0317, 0.0226, 0.1413, 0.2250, 0.1806, 0.2694])\n",
            "tensor([0.0800, 0.0000, 0.0400, 0.0000, 0.0000, 0.2200, 0.2200, 0.1800, 0.2600])\n",
            "the best pi is : -inf\n",
            "40/50\n",
            "\n",
            "Policy Loss 0.9019894480705262\n",
            "Value Loss 0.016726812347769737\n",
            "Examples:\n",
            "tensor([0.0151, 0.0072, 0.0478, 0.0315, 0.7919, 0.0268, 0.0166, 0.0249, 0.0382])\n",
            "tensor([0.0200, 0.0000, 0.0000, 0.0000, 0.9800, 0.0000, 0.0000, 0.0000, 0.0000])\n",
            "\n",
            "Policy Loss 0.8192343056201935\n",
            "Value Loss 0.011497543193399906\n",
            "Examples:\n",
            "tensor([0.0182, 0.0196, 0.8087, 0.0200, 0.0241, 0.0391, 0.0093, 0.0480, 0.0129])\n",
            "tensor([0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
            "the best pi is : -inf\n",
            "41/50\n",
            "\n",
            "Policy Loss 1.0663069128990172\n",
            "Value Loss 0.007202805113047361\n",
            "Examples:\n",
            "tensor([0.0625, 0.0330, 0.0445, 0.0498, 0.0304, 0.1787, 0.1545, 0.1941, 0.2527])\n",
            "tensor([0.0400, 0.0000, 0.0400, 0.0000, 0.0000, 0.2400, 0.1800, 0.2200, 0.2800])\n",
            "\n",
            "Policy Loss 1.0584480702877044\n",
            "Value Loss 0.00637845741584897\n",
            "Examples:\n",
            "tensor([0.0093, 0.0028, 0.0328, 0.0104, 0.9167, 0.0072, 0.0049, 0.0039, 0.0120])\n",
            "tensor([0.0200, 0.0000, 0.0000, 0.0000, 0.9800, 0.0000, 0.0000, 0.0000, 0.0000])\n",
            "the best pi is : -inf\n",
            "42/50\n",
            "\n",
            "Policy Loss 0.7020381808280944\n",
            "Value Loss 0.007494422420859337\n",
            "Examples:\n",
            "tensor([0.0116, 0.0121, 0.8639, 0.0138, 0.0108, 0.0378, 0.0048, 0.0395, 0.0056])\n",
            "tensor([0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
            "\n",
            "Policy Loss 0.7010535597801208\n",
            "Value Loss 0.00716646034270525\n",
            "Examples:\n",
            "tensor([0.0458, 0.0077, 0.0079, 0.0112, 0.0084, 0.0410, 0.1258, 0.0512, 0.7010])\n",
            "tensor([0.0000, 0.0000, 0.0200, 0.0000, 0.0000, 0.0000, 0.1800, 0.0400, 0.7600])\n",
            "the best pi is : -inf\n",
            "43/50\n",
            "\n",
            "Policy Loss 0.6596315622329711\n",
            "Value Loss 0.005458330735564232\n",
            "Examples:\n",
            "tensor([0.0551, 0.0139, 0.0352, 0.0258, 0.0231, 0.2099, 0.1920, 0.2426, 0.2024])\n",
            "tensor([0.0400, 0.0000, 0.0400, 0.0000, 0.0000, 0.2400, 0.2000, 0.2400, 0.2400])\n",
            "\n",
            "Policy Loss 0.6379795014858246\n",
            "Value Loss 0.00460562186781317\n",
            "Examples:\n",
            "tensor([2.0071e-02, 9.4821e-01, 7.1219e-04, 1.8579e-03, 6.5964e-04, 1.1517e-02,\n",
            "        6.5723e-03, 4.2616e-03, 6.1361e-03])\n",
            "tensor([0.0200, 0.9800, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])\n",
            "the best pi is : -inf\n",
            "44/50\n",
            "\n",
            "Policy Loss 0.6217350006103516\n",
            "Value Loss 0.0032326166052371263\n",
            "Examples:\n",
            "tensor([1.4922e-02, 9.6258e-01, 4.6909e-04, 1.0348e-03, 4.2835e-04, 8.7113e-03,\n",
            "        4.5399e-03, 3.1192e-03, 4.1949e-03])\n",
            "tensor([0.0200, 0.9800, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])\n",
            "\n",
            "Policy Loss 0.6424401521682739\n",
            "Value Loss 0.003056177345570177\n",
            "Examples:\n",
            "tensor([9.7899e-01, 4.7234e-03, 8.3679e-04, 1.3557e-03, 3.7992e-04, 3.7077e-03,\n",
            "        2.2721e-03, 3.9964e-03, 3.7357e-03])\n",
            "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "the best pi is : -inf\n",
            "45/50\n",
            "\n",
            "Policy Loss 0.5894999384880066\n",
            "Value Loss 0.002616599970497191\n",
            "Examples:\n",
            "tensor([0.9429, 0.0087, 0.0035, 0.0043, 0.0021, 0.0111, 0.0072, 0.0096, 0.0106])\n",
            "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "\n",
            "Policy Loss 0.6154761880636215\n",
            "Value Loss 0.0033073407248593867\n",
            "Examples:\n",
            "tensor([0.0224, 0.9360, 0.0021, 0.0023, 0.0014, 0.0119, 0.0086, 0.0059, 0.0093])\n",
            "tensor([0.0200, 0.9800, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])\n",
            "the best pi is : -inf\n",
            "46/50\n",
            "\n",
            "Policy Loss 0.5927950024604798\n",
            "Value Loss 0.0018062931951135397\n",
            "Examples:\n",
            "tensor([0.0172, 0.9502, 0.0014, 0.0015, 0.0010, 0.0101, 0.0067, 0.0039, 0.0078])\n",
            "tensor([0.0200, 0.9800, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])\n",
            "\n",
            "Policy Loss 0.5591326475143432\n",
            "Value Loss 0.0015620860853232444\n",
            "Examples:\n",
            "tensor([0.0085, 0.0011, 0.0148, 0.0028, 0.9641, 0.0020, 0.0012, 0.0026, 0.0029])\n",
            "tensor([0.0200, 0.0000, 0.0000, 0.0000, 0.9800, 0.0000, 0.0000, 0.0000, 0.0000])\n",
            "the best pi is : -inf\n",
            "47/50\n",
            "\n",
            "Policy Loss 0.569355571269989\n",
            "Value Loss 0.0012025763862766325\n",
            "Examples:\n",
            "tensor([0.0475, 0.0251, 0.0253, 0.5632, 0.0099, 0.1632, 0.0209, 0.0864, 0.0584])\n",
            "tensor([0.0200, 0.0200, 0.0200, 0.6200, 0.0000, 0.1600, 0.0200, 0.0800, 0.0600])\n",
            "\n",
            "Policy Loss 0.5772424101829529\n",
            "Value Loss 0.0012414905009791254\n",
            "Examples:\n",
            "tensor([0.0273, 0.0048, 0.0223, 0.0092, 0.9068, 0.0068, 0.0051, 0.0077, 0.0100])\n",
            "tensor([0.0200, 0.0000, 0.0000, 0.0000, 0.9800, 0.0000, 0.0000, 0.0000, 0.0000])\n",
            "the best pi is : -inf\n",
            "48/50\n",
            "\n",
            "Policy Loss 0.5901063561439515\n",
            "Value Loss 0.000978757702978328\n",
            "Examples:\n",
            "tensor([0.0173, 0.0016, 0.0088, 0.0041, 0.9568, 0.0025, 0.0020, 0.0031, 0.0037])\n",
            "tensor([0.0200, 0.0000, 0.0000, 0.0000, 0.9800, 0.0000, 0.0000, 0.0000, 0.0000])\n",
            "\n",
            "Policy Loss 0.5660515815019608\n",
            "Value Loss 0.0009154828469036147\n",
            "Examples:\n",
            "tensor([1.9254e-03, 1.1627e-03, 9.9036e-01, 8.1821e-04, 2.0234e-03, 1.6268e-03,\n",
            "        2.8886e-04, 1.3302e-03, 4.6460e-04])\n",
            "tensor([0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
            "the best pi is : -inf\n",
            "49/50\n",
            "\n",
            "Policy Loss 0.569740641117096\n",
            "Value Loss 0.0006858226493932307\n",
            "Examples:\n",
            "tensor([0.0207, 0.0073, 0.0438, 0.0067, 0.0103, 0.4105, 0.1741, 0.2090, 0.1174])\n",
            "tensor([0.0200, 0.0000, 0.0400, 0.0000, 0.0000, 0.3600, 0.2000, 0.2200, 0.1600])\n",
            "\n",
            "Policy Loss 0.5521808862686157\n",
            "Value Loss 0.0006859943037852645\n",
            "Examples:\n",
            "tensor([0.0328, 0.0247, 0.0289, 0.5132, 0.0087, 0.1735, 0.0299, 0.1081, 0.0803])\n",
            "tensor([0.0200, 0.0200, 0.0200, 0.6200, 0.0000, 0.1600, 0.0200, 0.0800, 0.0600])\n",
            "the best pi is : -inf\n",
            "50/50\n",
            "\n",
            "Policy Loss 0.5939900040626526\n",
            "Value Loss 0.00043725846335291864\n",
            "Examples:\n",
            "tensor([0.0194, 0.0019, 0.0121, 0.0039, 0.9503, 0.0030, 0.0020, 0.0029, 0.0044])\n",
            "tensor([0.0200, 0.0000, 0.0000, 0.0000, 0.9800, 0.0000, 0.0000, 0.0000, 0.0000])\n",
            "\n",
            "Policy Loss 0.549138754606247\n",
            "Value Loss 0.00040260391251649706\n",
            "Examples:\n",
            "tensor([0.0112, 0.0037, 0.0213, 0.0050, 0.0046, 0.4264, 0.1911, 0.2177, 0.1191])\n",
            "tensor([0.0200, 0.0000, 0.0200, 0.0000, 0.0000, 0.4200, 0.2000, 0.2200, 0.1200])\n",
            "the best pi is : -inf\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "args = {\n",
        "    'batch_size': 64,\n",
        "    'numIters': 50,                                # Total number of training iterations\n",
        "    'num_simulations': 50,                         # Total number of MCTS simulations to run when deciding on a move to play\n",
        "    'numEps': 50,                                  # Number of full games (episodes) to run during each iteration\n",
        "    'numItersForTrainExamplesHistory': 20,\n",
        "    'epochs': 2,                                    # Number of epochs of training per iteration\n",
        "    'checkpoint_path': 'latest.pth'                 # location to save latest set of weights\n",
        "}\n",
        "\n",
        "game = TicTacToeGame()\n",
        "board_size = game.get_board_size()\n",
        "action_size = game.get_action_size()\n",
        "\n",
        "model = TicTacToeModel(board_size, action_size)\n",
        "\n",
        "trainer = Trainer(game, model, args)\n",
        "final_losses_pi, final_losses_v, reward_player_X, reward_player_O = trainer.learn()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2OsNoebBotV",
        "outputId": "5c2b095c-72bc-450c-de77-517f1ac9232b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Losses of the policy distribution')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(final_losses_pi)\n",
        "plt.title('Losses of the policy distribution')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xo590ZBKKED2",
        "outputId": "0e1940ee-7df5-4d5c-ea24-61c8857542c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Losses of the values estimations')"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(final_losses_v)\n",
        "plt.title('Losses of the values estimations')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}